import numpy as np
import pickle
import statistics
from scipy.stats import skew
from collections import defaultdict
import pandas as pd
import gzip
import gc
import os


class PlantCCCPostProcessor:
    """
    - ç”Ÿæˆç»†èƒé€šä¿¡åˆ†æç»“æœ
    """

    def __init__(self, data_name, model_name,
                 embedding_path='embedding_data/',
                 metadata_from='metadata/',
                 data_from='input_graph/',
                 output_path='output/',
                 top_percent=20):

        self.data_name = data_name
        self.model_name = model_name
        self.embedding_path = os.path.join(embedding_path, data_name)
        self.metadata_from = os.path.join(metadata_from, data_name)
        self.data_from = os.path.join(data_from, data_name)
        self.output_path = os.path.join(output_path, data_name)
        self.top_percent = top_percent

        self.barcode_info = None
        self.lig_rec_dict = None
        self.row_col = None  # shape: (E,2) æŒ‰é¢„å¤„ç†ä¿å­˜é¡ºåº
        self.lig_rec = None  # len=E, æ¯æ¡è¾¹çš„ (ligand,receptor)
        self.total_num_cell = None

        self.results = {
            'scores': [],
            'edges': [],
            'raw_scores': {},
            'lr_pairs': set()
        }

        self._create_output_dir()
        self._validate_attention_file()

    def _create_output_dir(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(self.output_path, exist_ok=True)

    def _attention_file(self):
        """è·å–æ³¨æ„åŠ›æ–‡ä»¶è·¯å¾„"""
        return f"{self.embedding_path}/{self.model_name}_attention"

    def _validate_attention_file(self):
        """éªŒè¯æ³¨æ„åŠ›æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\n" + "=" * 70)
        print("éªŒè¯GATè¾“å‡ºæ–‡ä»¶...")
        print("=" * 70)

        attention_file = self._attention_file()
        if os.path.exists(attention_file):
            print(f"âœ… æ‰¾åˆ°æ³¨æ„åŠ›æ–‡ä»¶: {os.path.basename(attention_file)}")
        else:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ° æ³¨æ„åŠ›æ–‡ä»¶: {attention_file}\n"
                f"è¯·ç¡®è®¤è®­ç»ƒé˜¶æ®µè¾“å‡ºäº† {self.model_name}_attention æ–‡ä»¶"
            )

        print("=" * 70 + "\n")

    def load_metadata(self):
        """åŠ è½½å…ƒæ•°æ®ä¸L-Ré‚»æ¥ä¿¡æ¯ï¼ˆåŒæ—¶ä¿ç•™ edge-level æœ‰åºè®°å½•ï¼‰"""
        print("åŠ è½½å…ƒæ•°æ®...")

        barcode_file = os.path.join(self.metadata_from, f"{self.data_name}_barcode_info")
        with gzip.open(barcode_file, 'rb') as fp:
            self.barcode_info = pickle.load(fp)
        print(f"âœ… åŠ è½½äº† {len(self.barcode_info)} ä¸ªç»†èƒ")

        adjacency_file = os.path.join(self.data_from, f"{self.data_name}_adjacency_records")
        if not os.path.exists(adjacency_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°é‚»æ¥è®°å½•æ–‡ä»¶: {adjacency_file}")

        print("åŠ è½½é‚»æ¥è®°å½•...")
        with gzip.open(adjacency_file, 'rb') as fp:
            row_col, edge_weight, lig_rec, total_num_cell = pickle.load(fp)

        # âœ… å…³é”®ï¼šä¿ç•™â€œæœ‰åºâ€çš„ edge-level è®°å½•ï¼ˆç”¨äºä¸€ä¸€å¯¹é½ attentionï¼‰
        self.row_col = np.asarray(row_col, dtype=np.int64)  # (E,2)
        self.lig_rec = list(lig_rec)  # len=E, [(lig,rec),...]
        self.total_num_cell = int(total_num_cell)

        # ï¼ˆå¯é€‰ï¼‰ä»ç„¶ä¿ç•™ä½ åŸæ¥çš„ lig_rec_dictï¼šä»…ç”¨äº component/self-loop åˆ¤æ–­
        self.lig_rec_dict = defaultdict(lambda: defaultdict(list))
        for idx in range(len(row_col)):
            i = row_col[idx][0]
            j = row_col[idx][1]
            lr_pair = lig_rec[idx]
            if lr_pair not in self.lig_rec_dict[i][j]:
                self.lig_rec_dict[i][j].append(lr_pair)

        print(f"âœ… åŠ è½½äº† {len(self.row_col)} æ¡è¾¹ï¼ˆedge-levelï¼‰")
        print(f"âœ… æ¶‰åŠ {len(self.lig_rec_dict)} ä¸ªå‘é€ç»†èƒ\n")

        del row_col, edge_weight, lig_rec
        gc.collect()

    def process_attention(self):
        """ä¸¥æ ¼ä¿®å¤ç‰ˆï¼ˆP0-2ï¼‰ï¼šæŒ‰è®ºæ–‡è¯­ä¹‰å¯¹é½
        - edge-level ä¸¥æ ¼å¯¹é½ï¼šattention ä¸ (i,j,ligand,receptor) æŒ‰ edge idx ä¸€ä¸€å¯¹åº”
        - CCC åˆ†æ•°ä½¿ç”¨ Eq.(1) çš„ unnormalized attentionï¼ˆå…¨å±€ min-max åˆ° [0,1]ï¼‰
        - softmax attentionï¼ˆEq.(2)ï¼‰ä»…ç”¨äº debugï¼Œä¸ç”¨äº CCC ranking
        """
        from collections import defaultdict
        import gzip, pickle
        import numpy as np

        print("=" * 70)
        print("å¤„ç† GAT æ³¨æ„åŠ›æƒé‡ï¼ˆedge-level ä¸¥æ ¼å¯¹é½ç‰ˆï¼ŒP0-2è¯­ä¹‰å¯¹é½ï¼‰...")
        print("=" * 70)

        if self.row_col is None or self.lig_rec is None:
            raise RuntimeError("è¯·å…ˆè¿è¡Œ load_metadata()ï¼Œç¡®ä¿å·²åŠ è½½ row_col/lig_recã€‚")

        attention_file = self._attention_file()

        with gzip.open(attention_file, 'rb') as fp:
            attention_bundle = pickle.load(fp)

        # -------- å°å·¥å…·ï¼šå…¼å®¹ torch / numpy / list --------
        def _to_numpy(x):
            try:
                import torch
                if isinstance(x, torch.Tensor):
                    return x.detach().cpu().numpy()
            except Exception:
                pass
            if isinstance(x, np.ndarray):
                return x
            return np.asarray(x)

        def _reduce_to_edge_vector(att, E, name="attention"):
            """æŠŠ attention è§„æ•´æˆ shape=(E,) çš„ per-edge æ ‡é‡å‘é‡ï¼Œå¹¶å¤„ç† multi-head."""
            a = _to_numpy(att)
            a = np.squeeze(a)

            if a.ndim == 0:
                # å•ä¸ªæ ‡é‡ä¸åˆç†
                raise ValueError(f"{name} åªæœ‰ä¸€ä¸ªæ ‡é‡ï¼Œæ— æ³•å¯¹åº” E={E} æ¡è¾¹ã€‚shape={getattr(att, 'shape', None)}")

            if a.ndim == 1:
                if a.shape[0] != E:
                    raise ValueError(f"{name} ç»´åº¦ä¸åŒ¹é…ï¼šlen={a.shape[0]} vs E={E}ã€‚shape={a.shape}")
                return a.astype(np.float64)

            if a.ndim == 2:
                # å¸¸è§ä¸¤ç§ï¼š[E, H] æˆ– [H, E] æˆ– [E,1]
                if a.shape[0] == E:
                    return a.mean(axis=1).astype(np.float64)
                if a.shape[1] == E:
                    return a.mean(axis=0).astype(np.float64)
                raise ValueError(f"{name} æ˜¯äºŒç»´ä½†æ— æ³•åˆ¤æ–­å“ªä¸€ç»´æ˜¯ Eï¼šshape={a.shape}, E={E}")

            # æ›´é«˜ç»´ï¼šå°½é‡ squeeze åä» >2 è¯´æ˜ç»“æ„å¤æ‚ï¼Œç›´æ¥æŠ¥é”™æ›´å®‰å…¨
            raise ValueError(f"{name} ç»´åº¦è¿‡é«˜ï¼ˆ>2ï¼‰ï¼Œè¯·æ£€æŸ¥è®­ç»ƒç«¯ä¿å­˜æ ¼å¼ï¼šshape={a.shape}")

        # -------- è§£æ bundle --------
        if len(attention_bundle) >= 6:
            idx_l2 = attention_bundle[5]  # edge_index for layer2
            att_eq1 = attention_bundle[2]  # Eq.(1) unnormalized attentionï¼ˆæˆ–å…¶logitï¼‰
            att_softmax = attention_bundle[4]  # Eq.(2) softmax-normalized alphaï¼ˆä»…debugï¼‰
            edge_indices = _to_numpy(idx_l2)
            print("âœ… ä½¿ç”¨ bundle æ–°æ ¼å¼ï¼šidx_l2 + attention")
        else:
            raise ValueError("attention_bundle æ ¼å¼è¿‡æ—§/ä¸å®Œæ•´ï¼Œä¸æ”¯æŒä¸¥æ ¼å¯¹é½ã€‚")

        # edge_indices è§„æ•´æˆ (2, E)
        edge_indices = np.asarray(edge_indices)
        if edge_indices.shape[0] != 2:
            # æœ‰äº›å®ç°æ˜¯ (E,2)ï¼Œè¿™é‡Œåšä¸€æ¬¡è½¬ç½®å…œåº•
            if edge_indices.shape[1] == 2:
                edge_indices = edge_indices.T
            else:
                raise ValueError(f"edge_indices å½¢çŠ¶å¼‚å¸¸ï¼ŒæœŸæœ› (2,E) æˆ– (E,2)ï¼Œå®é™… {edge_indices.shape}")

        E_att = edge_indices.shape[1]
        E_meta = len(self.lig_rec)

        # -------- å…³é”®ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆéå¸¸é‡è¦ï¼‰--------
        if E_att != E_meta:
            raise ValueError(
                f"âŒ edge æ•°ä¸ä¸€è‡´ï¼šattentionè¾¹æ•°={E_att}, adjacency_recordsè¾¹æ•°={E_meta}ã€‚\n"
                f"è¿™é€šå¸¸æ˜¯è®­ç»ƒæ—¶ GAT å±‚å†…éƒ¨ add_self_loops/coalesce æ”¹å˜äº†è¾¹é›†åˆæˆ–é¡ºåºã€‚\n"
                f"è§£å†³æ–¹æ¡ˆï¼šè®­ç»ƒç«¯ä¿å­˜ edge_id å¹¶æŒ‰ edge_id å¯¹é½ï¼Œæˆ–ç¦ç”¨ä¼šæ”¹å˜è¾¹é›†åˆ/é¡ºåºçš„æ“ä½œã€‚"
            )

        meta_edge_index = np.asarray(self.row_col).T  # (2,E)
        if not np.array_equal(meta_edge_index, edge_indices):
            raise ValueError(
                "âŒ edge_index é¡ºåºä¸ adjacency_records ä¸ä¸€è‡´ï¼Œæ— æ³•ä¿è¯ lig_rec[idx] çš„ä¸¥æ ¼å¯¹é½ã€‚\n"
                "è¿™æ„å‘³ç€è®­ç»ƒè¿‡ç¨‹ä¸­è¾¹é¡ºåºå‘ç”Ÿäº†é‡æ’ï¼ˆå¸¸è§åŸå› ï¼šadd_self_loops + coalesceï¼‰ã€‚\n"
                "è¯·åœ¨è®­ç»ƒç«¯åšç¡¬ä¿®å¤ï¼šä¿å­˜ edge_id å¹¶æŒ‰ edge_id å¯¹é½åå†ä¿å­˜ attentionã€‚"
            )

        # -------- P0-2 æ ¸å¿ƒï¼šCCC åˆ†æ•°ç”¨ Eq.(1) unnormalized attention --------
        att_u = _reduce_to_edge_vector(att_eq1, E_att, name="att_eq1 (Eq.1 unnormalized)")
        att_n = _reduce_to_edge_vector(att_softmax, E_att, name="att_softmax (Eq.2 normalized)")

        # Eq.(1) ç†è®ºä¸Šç»è¿‡ tanh åº”åœ¨ [-1,1]ã€‚è‹¥æ˜æ˜¾è¶…å‡ºï¼Œè¯´æ˜è¿™é‡Œæ›´åƒ logitï¼Œè¡¥ tanh()
        if (att_u.max() > 1.0001) or (att_u.min() < -1.0001):
            att_u = np.tanh(att_u)
            print("â„¹ï¸ æ£€æµ‹åˆ° Eq.(1) attention è¶…å‡º[-1,1]ï¼Œå·²åœ¨åå¤„ç†ä¸­è¡¥ tanh() ä»¥å¯¹é½è®ºæ–‡å…¬å¼ã€‚")
        else:
            print("â„¹ï¸ Eq.(1) attention å·²åœ¨[-1,1]ï¼Œé»˜è®¤è®¤ä¸ºè®­ç»ƒç«¯å·²åš tanh()ã€‚")

        # å…¨å±€ min-max åˆ° [0,1]ï¼šè®ºæ–‡çš„ communication probability / ranking è¯­ä¹‰
        smin, smax = float(att_u.min()), float(att_u.max())
        den = (smax - smin) if (smax > smin) else 1.0
        scaled_all = (att_u - smin) / den

        print(f"âœ… CCC ä½¿ç”¨ Eq.(1) unnormalized attentionï¼ˆè¡¥tanhåï¼‰å¹¶å…¨å±€ç¼©æ”¾åˆ°[0,1]")
        print(
            f"   Eq.(1)èŒƒå›´: [{att_u.min():.4f}, {att_u.max():.4f}] -> scaledèŒƒå›´: [{scaled_all.min():.4f}, {scaled_all.max():.4f}]")
        print(f"   Eq.(2) softmaxèŒƒå›´(ä»…debug): [{att_n.min():.4f}, {att_n.max():.4f}]")

        # -------- ä¸¥æ ¼ä¸€ä¸€å¯¹åº”ï¼šç¬¬ idx æ¡è¾¹ -> lig_rec[idx] --------
        communication_dict = defaultdict(list)
        lr_pairs_set = set()

        for idx in range(E_att):
            i = int(edge_indices[0, idx])
            j = int(edge_indices[1, idx])

            ligand, receptor = self.lig_rec[idx]  # âœ… å…³é”®ï¼šåŒ idx çš„ lig_rec
            scaled = float(scaled_all[idx])

            key = f"{i}+{j}+{ligand}+{receptor}"
            communication_dict[key].append(scaled)
            lr_pairs_set.add((ligand, receptor))

        print(f"âœ… æœ‰æ•ˆé€šä¿¡é”®æ•°: {len(communication_dict)}ï¼ˆç†è®ºä¸Šâ‰ˆè¾¹æ•°E={E_att}ï¼‰")

        # -------- ç”Ÿæˆ edge_listï¼ˆrank æŒ‰ score é™åºï¼‰--------
        tmp = []
        raw_scores_dict = {}

        for key, scores in communication_dict.items():
            sc = float(np.mean(scores))
            tmp.append((key, sc))
            raw_scores_dict[key] = sc

        tmp.sort(key=lambda x: x[1], reverse=True)

        edge_list = []
        for rk, (k, sc) in enumerate(tmp, start=1):
            edge_list.append([k, rk, sc])

        self.results['edges'] = edge_list
        self.results['scores'] = [e[2] for e in edge_list]
        self.results['raw_scores'] = raw_scores_dict
        self.results['lr_pairs'] = lr_pairs_set

        # å¯é€‰ï¼šæŠŠ debug ä¿¡æ¯ä¹Ÿå­˜ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä½ æ’æŸ¥
        self.results['debug_att_eq1_scaled'] = scaled_all.tolist()
        self.results['debug_att_eq1_raw'] = att_u.tolist()
        self.results['debug_att_softmax_mean'] = att_n.tolist()

        print("âœ… å¤„ç†å®Œæˆï¼š")
        print(f"   é€šä¿¡æ•°: {len(edge_list)}")
        print(f"   Lâ€“Rå¯¹ç§ç±»: {len(lr_pairs_set)}")
        print(f"   åˆ†æ•°èŒƒå›´: [{min(self.results['scores']):.4f}, {max(self.results['scores']):.4f}]")

    def save_results(self):
        """ä¿å­˜ç»“æœï¼ˆæ¢å¤æ—§ç‰ˆCSVè¡¨å¤´ä¸componentè®¡ç®—ï¼‰
        è¾“å‡ºåˆ—ï¼š
          from_cell,to_cell,ligand,receptor,edge_rank,component,from_id,to_id,attention_score
        """
        import os
        import numpy as np
        import pandas as pd
        from collections import defaultdict
        from scipy.sparse import csr_matrix
        from scipy.sparse.csgraph import connected_components

        print("\n" + "=" * 70)
        print("ä¿å­˜ç»“æœï¼ˆæ—§ç‰ˆè¡¨å¤´ï¼‰.")
        print("=" * 70)

        edge_list = self.results['edges']  # [key, rank, score] å…¶ä¸­ scoreâˆˆ[0,1]ï¼Œå·²ç­‰ä»·æ—§ attention_score
        lr_pairs = self.results.get('lr_pairs', set())

        if not edge_list:
            print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
            return

        # ---------------- è®¡ç®— componentï¼ˆæŒ‰æ—§ç‰ˆé€»è¾‘ï¼‰ ----------------
        n = len(self.barcode_info)
        connecting = np.zeros((n, n), dtype=int)

        # ä»»ä¸€ (i,j, Lâ€“R) å­˜åœ¨å³è§†ä¸ºæœ‰è¾¹
        for key, _, _ in edge_list:
            i, j, _, _ = key.split('+')
            i = int(i);
            j = int(j)
            connecting[i, j] = 1

        # weak è¿æ¥åˆ†é‡
        graph = csr_matrix(connecting)
        n_comp, labels = connected_components(csgraph=graph, directed=True, connection='weak', return_labels=True)

        # å„åˆ†é‡å†…ç‚¹æ•°
        counts = np.zeros(n_comp, dtype=int)
        for lab in labels:
            counts[lab] += 1

        # å¤šç‚¹åˆ†é‡ç¼–å·ä»2å¼€å§‹
        comp_map = {}
        next_id = 2
        for cid in range(n_comp):
            if counts[cid] > 1:
                comp_map[cid] = next_id
                next_id += 1

        # ç»™æ¯ä¸ª cell èµ‹ component idï¼šå¤šç‚¹â†’2,3,...ï¼›è‡ªåˆ†æ³Œâ†’1ï¼›å…¶ä»–â†’0
        cell_component = [0] * n
        for i in range(n):
            if counts[labels[i]] > 1:
                cell_component[i] = comp_map[labels[i]]
            elif connecting[i, i] == 1 and (
                    i in self.lig_rec_dict and i in self.lig_rec_dict[i] and len(self.lig_rec_dict[i][i]) > 0):
                cell_component[i] = 1
            else:
                cell_component[i] = 0

        # ---------------- ç»„è£…CSVï¼ˆæ¢å¤æ—§ç‰ˆåˆ—åä¸å«ä¹‰ï¼‰ ----------------
        header = ['from_cell', 'to_cell', 'ligand', 'receptor', 'edge_rank', 'component', 'from_id', 'to_id',
                  'attention_score']
        records_all = [header]

        # ä¹Ÿé¡ºä¾¿åšä¸ª L-R ç»Ÿè®¡ï¼ˆä¸å½±å“ä¸»CSVï¼‰
        lr_stats = defaultdict(lambda: {'count': 0, 'total_score': 0.0})

        for key, rank, score in edge_list:
            i_str, j_str, ligand, receptor = key.split('+')
            i = int(i_str);
            j = int(j_str)

            comp_val = cell_component[i]
            if comp_val == 0:
                # æ—§ç‰ˆé‡åˆ°0ä¼šæ‰“å°errorå¹¶è·³è¿‡å†™å‡ºï¼›ä¿æŒå…¼å®¹
                # print('warning: component=0 at from_id', i)
                continue
            comp_field = '0-single' if comp_val == 1 else comp_val

            records_all.append([
                self.barcode_info[i][0],  # from_cell
                self.barcode_info[j][0],  # to_cell
                ligand,
                receptor,
                int(rank),  # edge_rankï¼ˆ1æ˜¯æœ€é«˜ï¼‰
                comp_field,  # component
                i,  # from_id
                j,  # to_id
                float(score)  # attention_scoreï¼ˆ0~1ï¼‰
            ])

            lr_stats[(ligand, receptor)]['count'] += 1
            lr_stats[(ligand, receptor)]['total_score'] += float(score)

        # ä¿å­˜ allCCCï¼ˆæ—§åä¸æ–°åå¹¶å­˜ä½ å¯æŒ‰éœ€ä¿ç•™ï¼‰
        out_all = os.path.join(self.output_path, f"{self.model_name}_allCCC.csv")
        pd.DataFrame(records_all[1:], columns=records_all[0]).to_csv(out_all, index=False)
        print(f"\nâœ… å·²ä¿å­˜: {os.path.basename(out_all)} ({len(records_all) - 1} æ¡)")

        # ä¿å­˜ Top N%
        top_n = max(1, int(len(records_all[1:]) * self.top_percent / 100))
        out_top = os.path.join(self.output_path, f"{self.model_name}_top{self.top_percent}percent.csv")
        pd.DataFrame(records_all[1:top_n + 1], columns=records_all[0]).to_csv(out_top, index=False)
        print(f"âœ… Top {self.top_percent}%: {os.path.basename(out_top)} ({top_n} æ¡)")

        # ï¼ˆå¯é€‰ï¼‰ä¿ç•™ä½ å½“å‰ç‰ˆæœ¬é‡Œå¯¹ L-R çš„ç»Ÿè®¡ä¸å»é‡åˆ—è¡¨
        lr_pairs_file = os.path.join(self.output_path, f"{self.model_name}_unique_LR_pairs.csv")
        with open(lr_pairs_file, 'w') as f:
            f.write("ligand,receptor\n")
            for ligand, receptor in sorted(lr_pairs):
                f.write(f"{ligand},{receptor}\n")
        print(f"âœ… L-Rå¯¹åˆ—è¡¨: {os.path.basename(lr_pairs_file)} ({len(lr_pairs)} ç§)")

        # è¯¦ç»†ç»Ÿè®¡ï¼ˆæ•°é‡ã€å‡åˆ†ç­‰ï¼‰
        lr_stats_rows = []
        for (lig, rec), info in lr_stats.items():
            cnt = info['count']
            tot = info['total_score']
            lr_stats_rows.append({
                'Ligand': lig,
                'Receptor': rec,
                'Communication_Count': cnt,
                'Avg_Score': f"{(tot / cnt) if cnt > 0 else 0:.6f}",
                'Total_Score': f"{tot:.6f}"
            })
        lr_stats_df = pd.DataFrame(lr_stats_rows).sort_values('Communication_Count', ascending=False)
        lr_stats_file = os.path.join(self.output_path, f"{self.model_name}_LR_pairs_statistics.csv")
        lr_stats_df.to_csv(lr_stats_file, index=False)
        print(f"âœ… L-Rå¯¹è¯¦ç»†ç»Ÿè®¡: {os.path.basename(lr_stats_file)} ({len(lr_stats_rows)} ç§)")

    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        print("=" * 70)

        scores = self.results['scores']
        lr_pairs = self.results['lr_pairs']

        if len(scores) == 0:
            print("âš ï¸ æ— æ•°æ®å¯ç»Ÿè®¡")
            return

        stats_path = os.path.join(self.output_path, f"{self.model_name}_statistics.txt")
        with open(stats_path, 'w') as f:
            f.write("=" * 70 + "\n")
            f.write("PlantCCC ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"æ•°æ®é›†: {self.data_name}\n")
            f.write(f"æ¨¡å‹å: {self.model_name}\n")
            f.write(f"ç»†èƒæ•°: {len(self.barcode_info)}\n\n")

            f.write("ç»Ÿè®¡:\n")
            f.write(f"  é€šä¿¡æ•°: {len(scores)}\n")
            f.write(f"  æ¶‰åŠçš„L-Rå¯¹æ•°: {len(lr_pairs)}\n")
            f.write(f"  åˆ†æ•°èŒƒå›´: [{np.min(scores):.6f}, {np.max(scores):.6f}]\n")
            f.write(f"  ä¸­ä½æ•°: {statistics.median(scores):.6f}\n")
            f.write(f"  å¹³å‡å€¼: {np.mean(scores):.6f}\n")
            f.write(f"  æ ‡å‡†å·®: {np.std(scores):.6f}\n")
            f.write(f"  ååº¦: {skew(scores):.6f}\n\n")

            f.write("=" * 70 + "\n")

        print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(stats_path)}")

        # æ§åˆ¶å°è¾“å‡º
        print(f"\nğŸ“Š  ç»Ÿè®¡æ‘˜è¦:")
        print(f"   é€šä¿¡æ•°: {len(scores)}")
        print(f"   L-Rå¯¹æ•°: {len(lr_pairs)}")
        print(f"   åˆ†æ•°èŒƒå›´: [{np.min(scores):.4f}, {np.max(scores):.4f}]")
        print(f"   å¹³å‡åˆ†æ•°: {np.mean(scores):.4f}")

    def run(self):
        """å®Œæ•´æµç¨‹"""
        print("\n" + "=" * 70)
        print("å¼€å§‹ PlantCCC åå¤„ç†")
        print("=" * 70 + "\n")

        self.load_metadata()
        self.process_attention()
        self.save_results()
        self.generate_statistics()

        print("\n" + "=" * 70)
        print("âœ… åå¤„ç†å®Œæˆï¼")
        print("=" * 70)


def main():
    RAW_DATA_ROOT = "../data/Arabidopsis/binned_outputs"
    DATA_NAME = 'square_016um'  # ä¸è¦å¸¦æ–œæ 
    MODEL_NAME = "Arabidopsis_model"
    # è¾“å‡ºæ ¹ç›®å½•
    OUTPUT_ROOT = "output/Arabidopsis_results"

    # è‡ªåŠ¨æ„å»ºå­ç›®å½• (ç¡®ä¿å„æ¨¡å—è¯»å†™è·¯å¾„ä¸€è‡´)
    PATH_CONF = {
        'metadata': os.path.join(OUTPUT_ROOT, 'metadata'),
        'input_graph': os.path.join(OUTPUT_ROOT, 'input_graph'),
        'embedding': os.path.join(OUTPUT_ROOT, 'embedding_data'),
        'model': os.path.join(OUTPUT_ROOT, 'model'),
        'vis_output': os.path.join(OUTPUT_ROOT, 'visualization')
    }

    # å®šä¹‰ä¿ç•™å‰ç™¾åˆ†ä¹‹å¤šå°‘çš„è¾¹
    TOP_PERCENT = 20

    post_processor = PlantCCCPostProcessor(
        data_name=DATA_NAME,
        model_name=MODEL_NAME,
        embedding_path=PATH_CONF['embedding'],  # è¯»å– Trainer çš„è¾“å‡º
        metadata_from=PATH_CONF['metadata'],  # è¯»å– Preprocessor çš„è¾“å‡º
        data_from=PATH_CONF['input_graph'],  # è¯»å– Preprocessor çš„è¾“å‡º
        output_path=PATH_CONF['vis_output'],  # ç»“æœä¿å­˜ä½ç½®
        top_percent=TOP_PERCENT
    )

    post_processor.run()


if __name__ == "__main__":

    main()
